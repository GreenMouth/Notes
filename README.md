# Notes


Hyperparameter Optimization: https://alykhantejani.github.io/images/gradient_descent_line_graph.gif

Gradient Descent: https://hackernoon.com/dl03-gradient-descent-719aff91c7d6

Chain Rule in differentiation (for backpropogation): http://www.mathcentre.ac.uk/resources/uploaded/mc-ty-chain-2009-1.pdf

Backpropogation paper 1996: http://www.iro.umontreal.ca/~pift6266/A06/refs/backprop_old.pdf

https://www.reddit.com/r/tensorflow/comments/dej9zd/visualizing_a_neural_network_created_using_tensor/
https://preview.redd.it/z516nyulc4r31.gif?format=mp4&s=7da7884ef4b131d8128a95fab800bee395b664ec

The tanh activation usually works better than sigmoid activation function for hidden units because the mean of its output is closer to zero, and so it centers the data better for the next layer. 

