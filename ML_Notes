
#Standardizing the input feature
#StandardScaler standardizes a feature by subtracting the mean and then scaling to unit variance.
#It will transform your data such that its distribution will have a mean value 0 and standard deviation of 1.
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X = sc.fit_transform(X)


Deploy models using OOP (object oriented programming) - use classes instead of function, etc.

Neural Networks Learning Steps: (Image)
layer 1: (Pixel to Edges) (input & hidden layer 1)
layer 2: edges to pattern (hidden layer 1 & hidden layer 2)
final_layer: pattern to image recognition

MNIST Dataset: Modified National Institute of Standards & Technology


In LINEAR Models: Ridge regularisation (L2 Regularisation) does not shrink the coefficients to zero. 
However, Lasso regularization (L1) shrinks the coefficient to ZERO. 

Random Forest:
RF consists of 100s of decision trees each of them built over a "random extraction of the observations" from the dataset and a
"random extraction of features".
Not every tree sess ALL observations or features hence trees are de-correlated and less prone to overfitting.
For classification, the measure of impurity is either the Gini impurity or the information gain/entropy.
For regression the measure of impurity is variance.
Feature Importance:  The more a feature decreases the impurity, the more important the feature is.

Feature Selection: from sklearn.feature_selection import SelectFromModel
SelectKBest or SelectPercentile from sklearn

